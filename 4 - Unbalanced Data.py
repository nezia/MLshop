# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v-rTv_w_c57Q80n9jgKU9JEVVynvfmXZ

Unbalanced Data

Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import tensorflow.keras as keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve

"""Import Fraud data"""

# Import Training Data
df = pd.read_csv('A2 - Fraud - Train.csv')
print(df.columns)
print(df.head())
features = df.values[:,:-1]
labels   = df.values[:,-1].astype('int')


# Import Test Features (no labels)
df_features_test = pd.read_csv('A2 - Fraud - Test Features.csv')
print(df_features_test.columns)
print(df_features_test.head())
features_test = df_features_test.values

"""Take a quick look at the data. Check to make sure that train and test features are reasonably similar.  Note the consistant and unexpected relationship between the two datasets for the 'V' variables V1-V28."""

# Check the data to make sure training & test features are roughly similar
m1 = np.mean(features,axis=0)
m2 = np.mean(features_test,axis=0)
meanscomparison = np.stack((m1, m2, m1/m2), axis=1)
np.set_printoptions(suppress=True)
print(meanscomparison)

sd1 = np.std(features,axis=0)
sd2 = np.std(features_test,axis=0)
stdcomparison = np.stack((sd1, sd2, sd1/sd2), axis=1)
print(stdcomparison)

meanscomparison2 = np.stack((m1, m2, (m1-m2)/sd1), axis=1)
print(meanscomparison2)

"""The features look fine across datasets.  When we normalize the data, we shouldn't have any problems.

Now lets look at the training labels
"""

# Check Labels
print(np.unique(labels, return_counts=True))
print(labels.mean())

"""Wow!  The labels are extremely skewed. ~0.17% of the data is class = 1 (fraud transactions)  and the other ~99.83% of the data is class 0 (e.g., not fraud).  We will need to be very careful when training our models to account for this large imbalance.

Lets create separate Train and Val (validation) data sets for our analysis. Note that we are using the 'stratify' option in train_test_split.  This ensures that we have the same distribution of class labels across the two datasets.  Given the very small number of fraud transactions, if we didn't do this, its possible that either of our data sets would have very very few fraud transactions and our models wouldn't train well. We will double-check to make sure this is the case.
"""

# Create Train & Validation sets (stratify on labels)
features_train, features_val, labels_train, labels_val = train_test_split(features, labels, train_size=0.80, shuffle=True, stratify=labels, random_state=314159)
#stratify <- same label ratio across dataset
print('Train Fraud rate:', np.mean(labels_train))
print(' Test Fraud rate:', np.mean(labels_val  ))

"""Now we need to normalize all of the features before we start training our models.  We will standardize ALL of the features to mean 0 and standard deviation = 1.  Note that we will standardize the training, test, and validation features using ONLY information from the training data.  We do this to avoid data leakage and reduce the generalization accuracy of our validation and test sets."""

# Normalize feature data according to TRAINING DATA
m_train  = np.mean(features_train, axis=0)
sd_train = np.std(features_train, axis=0)

features_train = (features_train - m_train)/sd_train
features_val   = (features_val   - m_train)/sd_train
features_test  = (features_test  - m_train)/sd_train

print(np.mean(features_train, axis=0))

print(np.mean(features_val, axis=0))

print(np.mean(features_test, axis=0))

print(np.std(features_train, axis=0))

print(np.std(features_val, axis=0))

print(np.std(features_test, axis=0))

"""Notice that the training data will have perfectly mean = 0 and std = 1, but the validation and test data will not.  This is to be expected.

Before we start, lets define a couple of functions that we will use later.
The plot_performance function will easily plot the loss and performance metric over epochs for our training and validation data after we fit a model.
The precision_recall_graph function will plot the precision vs. recall graph.
"""

def plot_performance(modelhist):
    temp = {k:modelhist.history[k] for k in modelhist.history.keys() if k in ['loss', 'val_loss']}
    sns.lineplot(data=temp, palette="tab10", linewidth=2.5)
    plt.show()
    temp = {k:modelhist.history[k] for k in modelhist.history.keys() if k in ['pr-auc', 'val_pr-auc']}
    sns.lineplot(data=temp, palette="tab10", linewidth=2.5)
    plt.show()
    return


def precision_recall_graph(y, yhat, best=True, label=None):
    precision, recall, _ = precision_recall_curve(y, yhat)
    if (best==True):
        for i in range(1,precision.shape[0]):
            if precision[i] < precision[i-1]:
                precision[i] = precision[i-1]
    plt.plot(recall, precision, label=label)
    plt.xlim(0,1)
    plt.ylim(0,1)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    if label!=None:
        plt.legend()
    #plt.show()
    return

"""Lets start with a simple baseline model, a logistic function (built using a neural network).  This will give us a good reference point going forward."""

def build_logistic_nn():
    # Specify model
    model = keras.Sequential()
    model.add(keras.layers.InputLayer(30))
    model.add(keras.layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform'))
    # Compile model
    opt = keras.optimizers.SGD(learning_rate=0.1)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', 'Precision', 'Recall', tf.keras.metrics.AUC(curve='PR', name='pr-auc')])
    return model


# Baseline
tf.random.set_seed(31415)
model_lnn = build_logistic_nn()
hist_lnn = model_lnn.fit(features_train, labels_train, epochs=20, batch_size=10000, verbose=0, validation_data=(features_val, labels_val))
print('Precision-Recall AUC Logistic:         ', hist_lnn.history['val_pr-auc'][-1])
predict_lnn = model_lnn.predict(features_val)
precision_recall_graph(labels_val, predict_lnn, label='Logistic', best=False)
plt.show()
precision_recall_graph(labels_val, predict_lnn, label='Logistic')
plt.show()

"""Our baseline Precision-Recall AUC is ~72.13%.  Additionally, note how the graphs capture the tradeoff between precision and recall.  Depending on whether recall or precision is more important, we can select a point along this cuve that best meets our situational needs.  Remember that a sigmoid activation function will simply assign classes based on a thresholf / cutoff of 0.5.  Choosing a different point along this graph is an example of thresholding.  If we didn't we would be stuck with the recall and precision that is at 0.5 in the first graph. This is clearly inferior in this case as we can select a higher threshold that increases BOTH recall and precision.

Now lets build a very simple single-layer dense neural network and see how it performs.  Note that I have choosen a higher batch_size than the default 32.  This is because larger batch sizes tend to work better here as small batches will likely have many batches that don't have fraudulent transactions, interfering with learning.  Again, too large might be problematic as well.
"""

def build_nn1():
    # Specify model
    model = keras.Sequential()
    model.add(keras.layers.InputLayer(30))
    model.add(keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal'))
    model.add(keras.layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform'))
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC(curve='PR', name='pr-auc')])
    return model

# Simple Model
tf.random.set_seed(31415)
model_1 = build_nn1()
hist_1 = model_1.fit(features_train, labels_train, epochs=10, batch_size=256, verbose=2, validation_data=(features_val, labels_val))
predict_1 = model_1.predict(features_val)
plot_performance(hist_1)
print('Precision-Recall AUC Standard:', hist_1.history['val_pr-auc'][-1])

"""Great! Our PR-AUC score already went up a lot with this very simple neural network.

Now, lets try the same model, but this time include weights that put more weight on the fraud transactions. We can either create a dictionary that maps labels to weights (as we do here using the 'class_weight' option in the fit function).  Alternatively create an array of weights that has the same number of observations as our data and uses those weights in the array for the respective observation (using the 'sample_weight' option in the fit function)
"""

# Create Weighting Dictionary mapping labels to weights
print(np.unique(labels_train, return_counts=True))
class_weights_dict = {0:1, 1:184920/320}

tf.random.set_seed(31415)
model_1W = build_nn1()
hist_1W = model_1W.fit(features_train, labels_train, epochs=10, batch_size=256, class_weight=class_weights_dict, verbose=2, validation_data=(features_val, labels_val))
predict_1W = model_1W.predict(features_val)
plot_performance(hist_1W)
print('Precision-Recall AUC Weighted:  ', hist_1W.history['val_pr-auc'][-1])

"""The PR-AUC is much lower for the weighted model. But this doesn't tell us the whole story.  Lets take a look at the curves themselves. It seems the weighted model performs better at very high recall, but the unweighted model performs better everywhere else."""

precision_recall_graph(labels_val, predict_1, label='Unweighted', best=True)
precision_recall_graph(labels_val, predict_1W, label='Weighted', best=True)
plt.show()

"""One way that we might be able to increase training speed and possibly our final model performance is by using smart bias initialization on output layer.  Remember that during the first step of training, the bias is set to zero. This means our model will likely classify about half of all transactions as fraud, even though we know the number is much much lower.  This means that our model will have to learn the bias at the beginning of training.  If we set a negative number as the bias, our model will initially classify fewer transactions as fraud, which should give us a better start.  Skipping the math, the theorhetical best bias initialization for the output layer here is the natural log of (# positive / # non-positive) or (# fraud / # non-fraud)."""

# Smart Initialization
#initial_bias = np.log([pos/neg])

initial_bias = np.log(np.sum(labels_train==1) / np.sum(labels_train==0))

tf.random.set_seed(31415)
model_1i = build_nn1()
model_1i.layers[-1].bias.assign([initial_bias])
hist_1i = model_1i.fit(features_train, labels_train, epochs=10, batch_size=256, verbose=2, validation_data=(features_val, labels_val))
predict_1i = model_1i.predict(features_val)
plot_performance(hist_1i)
print('Precision-Recall AUC Smart Initialized:  ', hist_1i.history['val_pr-auc'][-1])

""" Lets compare with the see if the smart initialization leads to any improvement."""

# Compare Initialization
plt.plot(hist_1.history['loss'])
plt.plot(hist_1i.history['loss'])
plt.yscale('log')
plt.show()

precision_recall_graph(labels_val, predict_1, label='Normal', best=True)
precision_recall_graph(labels_val, predict_1i, label='Smart Initialization', best=True)
plt.show()

"""We can see that the smart initialization led to much lower loss at the beginning of training.  Oftentimes, this will lead to a better final model. The smart initialization seems to improve performance at high recall, but might be slightly worse at lower recall.

Data-Based Approaches

Lets try undersampling the non-fraud transactions in our training data and see if that leeds to any improvement. Lets try a 20-to-1 ratio of Non-fraud to fraud transactions.  This is an arbitrary choice and normally you would experiment and include this number as another hyperparameter.
"""

ids = np.arange(features_train.shape[0])
ids_pos = ids[labels_train==1]
ids_neg = ids[labels_train==0]

ratio = 20
ids_neg_under = np.random.choice(ids_neg, size=ids_pos.shape[0]*ratio, replace=False)
index_undersampled = np.concatenate((ids_pos, ids_neg_under), axis=0)
np.random.shuffle(index_undersampled)  # shuffles in place

features_train_undersampled = features_train[index_undersampled,:]
labels_train_undersampled   = labels_train[index_undersampled]
print(np.unique(labels_train_undersampled, return_counts=True))

"""Lets train the same model with this undersampled data.  Due to the imbalance, even with a 20-to-1 ratio, we are left with only 6720 total transactions to train with.  This means we will need to increase the number of epochs to counterbalance the much smaller number of transactions in this undersampled data."""

tf.random.set_seed(31415)
model_1U = build_nn1()
hist_1U = model_1U.fit(features_train_undersampled, labels_train_undersampled, epochs=100, batch_size=256, verbose=0, validation_data=(features_val, labels_val))
predict_1U = model_1U.predict(features_val)
plot_performance(hist_1U)
print('Precision-Recall Undersampled:  ', hist_1U.history['val_pr-auc'][-1])

"""Now lets build an oversampled

Lets try oversampling the fraud transactions in our training data and see if that leeds to any improvement. Lets again aim for a 20-to-1 ratio of Non-fraud to fraud transactions.  Again, this is an arbitrary choice and could be potentially improved.
"""

# Oversampling
fraction = 0.05
ids_pos_over = np.random.choice(ids_pos, size=np.floor(ids_neg.shape[0]*fraction).astype('int'), replace=True)
index_oversampled = np.concatenate((ids_pos_over, ids_neg), axis=0)
np.random.shuffle(index_oversampled)  # shuffles in place

features_train_oversampled = features_train[index_oversampled,:]
labels_train_oversampled   = labels_train[index_oversampled]
print(np.unique(labels_train_oversampled, return_counts=True))

"""And lets train a model with this oversampled data"""

tf.random.set_seed(31415)
model_1O = build_nn1()
hist_1O = model_1O.fit(features_train_oversampled, labels_train_oversampled, epochs=10, batch_size=256, verbose=2, validation_data=(features_val, labels_val))
predict_1O = model_1O.predict(features_val)
plot_performance(hist_1O)
print('Precision-Recall Oversampled:  ', hist_1U.history['val_pr-auc'][-1])

"""To finish, lets take a quick look and compare the PR curves"""

# Comparison
precision_recall_graph(labels_val, predict_lnn, label='Baseline', best=True)
precision_recall_graph(labels_val, predict_1W, label='Weighted', best=True)
precision_recall_graph(labels_val, predict_1U, label='Undersampled', best=True)
precision_recall_graph(labels_val, predict_1O, label='Oversampled', best=True)